---
title: "Exploring Dataset Sonification with Web Audio"
date: "2023-06-10"
keywords:
  [
    "web",
    "audio",
    "data",
    "sonification",
    "javascript",
    "visuals",
    "nodejs",
    "computing",
    "analysis",
    "machine",
    "learning",
    "open",
    "source",
  ]
type: "post"
summary: "Creating audio-visual art on the Web is easier than ever, thanks to modern browser tech and open source creative coding libraries. In this post, I share two examples of how to enrich your dataset visualizations with model-based sonification in a web browser using the p5js library."
---

IMAGE

Creating audio-visual art on the Web is easier than ever, thanks to the growing capabilities of modern web browsers and the availability of open source creative coding libraries. Together, they facilitate an environment where people with varying technical abilities and backgrounds can create meaningful and interactive art in the browser at record speeds.

As an audio enthusiast, I have been interested in the audio capabilities of these libraries and how Web Audio can contribute to modern artistic and research practices. In early 2023, I ran a workshop in collaboration with the <MyLink href="http://c2ho.no">Creative Computing Hub Oslo (C2HO)</MyLink>, specifically on how sonification can help us navigate and explore Big data on the Web.

In this post, I share some examples from my workshop, presenting two approaches to model-based sonification of large datasets in a web browser using the <MyLink href="https://p5js.org/">p5js</MyLink> library.

# Contents

1. <MyLink href="#follow-the-code">Follow The Code</MyLink>
2. <MyLink href="#why-sonify?">Why Sonify?</MyLink>
3. <MyLink href="#multi-dimensional-graph">Multi-dimensional Graph</MyLink>
4. <MyLink href="#object-oriented-approach">Object-oriented Approach</MyLink>
5. <MyLink href="#summary">Summary</MyLink>
6. <MyLink href="#summary">Source Code</MyLink>

<hr />

# Follow The Code

If you want to follow my code examples, you can download the repository from my <MyLink href="https://github.com/aleksati/dataset-sonification-in-browser-workshop">GitHub</MyLink>. I recommend using the <MyLink href="https://editor.p5js.org/">p5js Web Editor</MyLink> as your development environment so you can focus 100% on coding. However, you can also run the code from a local node development server, if you want.

In any case, ensure that you use a Chromium-based web browser, such as Chrome, Brave, or Vivaldi. I have not experimented with other browser engines, such as Firefox's Gecko, so you might run into unforseen issues there.

To set up the development environment with p5, follow these simple steps:

1. Download the code repository as a .zip file.
2. Create an account on the p5js Web Editor.
3. Go to "My Ses" (click on "Hello, username!" on the top right) and create a new s.
4. In the web editor of your new s, open the sidebar (black arrow on gray backgroun) to access your files.
5. Via the small dropdown menu "S Files" in the left column, choose to "upload file" and upload the folder entitled "browser/examples".

<MyImage
  src="datason-p5-editor.png"
  caption="p5js Web Editor in a Brave browser tab anno june 2023."
  alt="p5js web editor"
/>

**Important note:** the p5js Web Editor might change over time, meaning the above description might not be 100% accurate.

# Why Sonify?

Perhaps the most common approach to engaging with data online is through visual graphs and plots where various axis represent different data dimensions. However, this is far from the only way that we can interact with and explore data online. The term Sonification, or Auditory Displays, refers to the use of non-speech audio to convey information or perceptualize data (Hermann et.al, 2011). Using sonification methods can facilitate better accessibility practices and sometimes even be more efficient than data visualization techniques.

IMAGE

The research literature often differentiates among methods of sonification. For instance, the method of Audification refers to the direct playback of data samples or a direct translation of a data waveform into sound. So with audification, we can perhaps perceive patterns and irregularities in large datasets in record time.

However, most sonification applications augment existing visual-based data analysis tools, helping to extract valuable and coherent information in realtime. Model-based sonification is another and more advanced method where the goal is to investigate how acoustic responses from dynamic (sometimes visual) environments can provide valuable information in response to user interactions. As such, model-based sonification systems are great for explorative data analysis and augmenting existing visual analysis methods.

# Multi-dimensional Graph

Below is my first example of a model-based sonification system where I've augmented a traditional 2D plot with web audio. The height (vertical Y-axis) of the red dots equals coffee acidity levels from different farms over time (horizontal X-axis). When you click the dots, the frequency of the played synth note represents the altitude of the farm. The higher the frequency of the note, the higher up the farm is.

The dataset I am using is a simple and generic csv spreadsheet detailing coffee quality from many different regions and farms over several years, found on Kaggle.com. For my example, I am only using a fraction of the dataset for demonstration purposes.

<P5SonifyPlot />

One of the great things about p5 is that it includes a ton of Web Audio API utility functions for high-level declarative audio programming. This framework makes it easy to define and update audio parameters on the fly, making it ideal for designing interactive model-based sonification systems for the Web.

In the code block below, I define an example p5 sketch with a 200x200 canvas and a simple sinewave oscillator. The frequency of the sinewave is gradually incremented (from 0 to 10kHz) on every new window frame, resulting in a continuous sine sweep.

```Javascript
// sinesweep sketch
const sketch = (p) => {
  let sine;
  let myFreq;
  let threshold = 10000;

  p.setup = () => {
    p.createCanvas(200, 200);
    loadAudio();
  };

  p.draw = () => {
    updateAudio();
  };

  loadAudio() => {
    // load an oscillator with a sine-wave and set initial amplitude
    sine = new p5.Oscillator("sine");
    sine.amp(0.5);
    sine.freq(0);
    sine.start();
  }

  updateAudio() => {
    // update the frequency of the sine on every new window frame
    myFreq = myFreq > threshold ? 0 : myFreq += 1;
    sine.freq(myFreq);
  }
};

let myp5 = new p5(sketch);
```

Let's take this idea of an augmented 2D plot one step further.

now that ... we can multi-dimensional approach.

data dimensions to audio

If we go back to our coffee quality example,

If we take this idea a little further ..
IF we plot, we could assign another coloumn.
You have to show some code...
audio.

In this example, we see coffee... taken from Kaggle.

In the r provided on GitHub, the first three js files, entited <i>template_1</i>, <i>template_2</i>, and <i>template_3</i>

are

Model-based sonification is ...

For the actual data

The dataset used in the examples in downloaded from Kaggle.

On Kaggle, you can find large good quality datasets to use for you projects.
can use anything you have, as long as the data in multi-dimension and preferably based on time-series

By interacting with the model environment, you gain important information about the dataset via sound.

Then, the approach.

X and Y (+++ color, size), sound .. also simple LFO for double.

# Object-oriented Approach

# Summary

# Source Code

# References

- Hermann, T., Hunt, A., & Neuhoff, J. G. (Eds.). (2011). The Sonification Handbook (1st ed.). Logos Publishing House.
