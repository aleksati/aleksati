---
title: "Using Knowledge Distillation to Compress Neuural Audio Effects: Making VSTs more efficient by Giving Them the Right Teachers"
date: "2025-09-06"
keywords: ["audio", "fx", "research", "machine-learning", "python", "latency", "realtime", "engineering"]
type: "post"
summary: "somethinbg"
---



Making Neural Audio Effects to Learn From Their Parents Using Knowledge Distillation



Making distortion plugins smaller and more intelligent by forcing them learn from their parents: a First Exploration on Knowledge Distillation to Compress Neural Audio Effect


Making small audio FX smaller and more intelligent by learning from their parents 

Using Knowledge Distillation to Compress Neuural Audio Effects: of audio effects: 

Using Knowledge Distillation to compress neural network models of audio FX



The combination of music and Ai has been big news for a while. Trendy as can be. A match made i heaven, they say. But every time I reflect on it I can't help but think about an image I saw not too long ago. The image was of a graph that depicted how the popularity of new technologies almost always tend to receed over time. The theory was that our expectations are almost always wrong. That the rapid increase in popularity and hype we many times witness almost always decreases and stabilizes at a sober level. And the graph I saw was hinting that this might happen to Ai as well. That we might currently be at the very peak of the hype curve waiting for the imminent fall. But this is not a statement about whether Ai will "fail" or not. It's more about saying that it will not be long before we better understand what AI can and cannot do, and for which fields and applications it will be worth pursuing. 

I am currently sitting in a damp and uncomfortable conference room at Queen Mary University College in London, listening to a talk about AI ethics at the first (yes, the first) international conference on AI and Machine Learning for Audio, hosted by the Audio Engineering Society (AES AIMLA) in September 2025. The question about how AI will affect the music industry is up in the air and a man from the team that originally developed the mp3 codec raises his hand and proposes that there might be some similarities between how we feared mp3 would ruin the music industry to how we today fear that AI will do the same. Although I believe that this dismisses many of the very real concerns we face when using AI in the field of music, and I am paraphrasing alot here, it does play into my thoughts about the anticipated hype-decreasing event that awaits our near future. 

But my opinion is probably as good as any's. What's important is that the issues are addressed and that people disagree. The whole reason I am even in London at an AES conference is because I was lucky enough be to involved in a research paper that got accepted here. This all started in 2024 when my collegue and friend Riccardo Simionato asked me if I wanted to help him explore if a process called Knowledge Distillation (KD) could make neural audio effects (audio effects built with neural networks) smaller and more efficient. KD is a compression technique where the _knowledge_, or features, in a large neural network are transfered to smaller networks. Essentially.

In the following post, I try to distill the knowledge of our AES paper and KD explorations into a simpler and more readable blog post format. To read the paper, listen to sound examples and view the source code, you can visit the dedicated <MyLink href="https://github.com/aleksati/kd-audio-fx">GitHub repository</MyLink>. 

# VA Modeling and Audio Effects Compression 

Neural networks and machine learning have proven to be very effective for researchers in the field of Virtual-Analog (VA) modeling. This are fairly recent discoveries, and what these VA researcher aim to do is to accuractly emulate analog audio effects in the digital domain, creating neural audio effects, VSTs and plugins. The real advantage of using Ai for VA modeling comes down to the exceptional ability of machine learning to find patterns in large quantities of data. Analog audio effects consists of complex hardware circuits that shape the sound in a unpredictable and _non-linear_ way. These non-linearities are hard to define mathamatically on a computer. With machine-learning, we can essentially skip the hard part and train models to recreate the analog behaviour without us actually knowing whats going on. 

This approach of treating the analog audio effect as a "black box" is called a black-box modeling.

In contrast, white-box modeling seek to .... mathmatically 

This technique is often refferd to Virtual-Analog (VA) modeling and is often comprised of either black-box and white-box modeling techique.


However, not many of the VA research that exist in this area guarantee lightweight solutions suitable for real-time environments where the models must run smoothly on consumer-grade devices (laptops, phones, etc.)... to be able to be used, they need to be small with low latency/

This is what sparked my collegue Riccardo to question whether a new and interesting compression technique called Knowledge Distillation could be a viable for audio effects domain. 


The whole idea produce for with high accuracy that maintain compact model size. 



This paper explores knowledge distillation techniques for compressing recurrent neural network models for audio distortion effects, aiming to produce computationally efficient solutions with high accuracy that maintain compact model size. 


# Knowledge Distillation 

