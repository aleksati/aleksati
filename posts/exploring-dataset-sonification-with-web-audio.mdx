---
title: "Exploring Dataset Sonification with Web Audio"
date: "2023-06-10"
keywords:
  [
    "web",
    "audio",
    "data",
    "sonification",
    "javascript",
    "visuals",
    "nodejs",
    "computing",
    "analysis",
    "machine",
    "learning",
    "open",
    "source",
  ]
type: "post"
summary: "Thanks to modern browser tech and open source creative coding libraries, creating audio-visual art on the Web is easier than ever. In this post, I share two examples of how to enrich your dataset visualizations with model-based sonification in a web browser using the p5js library."
---

IMAGE

Creating audio-visual art on the Web is easier than ever, thanks to the growing capabilities of modern web browsers and the availability of open source creative coding libraries. Together, they facilitate an environment where people with varying technical abilities and backgrounds can create meaningful and interactive art in the browser at record speeds.

As an audio enthusiast, I have been interested in the audio capabilities of these libraries for some time, and speficially how Web Audio can contribute to modern artistic and research practices. In early 2023, I ran a workshop in collaboration with the <MyLink href="http://c2ho.no">Creative Computing Hub Oslo (C2HO)</MyLink>, specifically on how sonification can help us navigate and explore Big data on the Web.

In this post, I share some examples from my workshop, presenting two approaches to model-based sonification of large datasets in a web browser using the <MyLink href="https://p5js.org/">p5js</MyLink> library.

# Contents

1. <MyLink href="#follow-the-code">Follow The Code</MyLink>
2. <MyLink href="#why-sonify?">Why Sonify?</MyLink>
3. <MyLink href="#dataset">Dataset</MyLink>
4. <MyLink href="#multi-dimensional-plot">Multi-dimensional Plot</MyLink>
5. <MyLink href="#the-sonic-orbs">The Sonic Orbs</MyLink>
6. <MyLink href="#summary">Summary</MyLink>
7. <MyLink href="#source-code">Source Code</MyLink>

<hr />

# Follow The Code

If you want to follow my code examples, you can download the repository from my <MyLink href="https://github.com/aleksati/dataset-sonification-in-browser-workshop">GitHub</MyLink>. I recommend using the <MyLink href="https://editor.p5js.org/">p5js Web Editor</MyLink> as your development environment so you can focus 100% on coding. However, you can also run the code from a local node development server, if you want.

In any case, ensure that you use a Chromium-based web browser, such as Chrome, Brave, or Vivaldi. I have not experimented with other browser engines, such as Firefox's Gecko, so you might run into unforseen issues there.

To set up the development environment with p5, follow these simple steps:

1. Download the code repository as a .zip file.
2. Create an account on the p5js Web Editor.
3. Go to "My Ses" (click on "Hello, username!" on the top right) and create a new s.
4. In the web editor of your new s, open the sidebar (black arrow on gray backgroun) to access your files.
5. Via the small dropdown menu "S Files" in the left column, choose to "upload file" and upload the folder entitled "browser/examples".

<MyImage
  src="datason-p5-editor.png"
  caption="p5js Web Editor in a Brave browser tab anno june 2023."
  alt="p5js web editor"
/>

**Important note:** the p5js Web Editor might change over time, meaning the above description might not be 100% accurate.

# Why Sonify?

Perhaps the most common approach to engaging with data online is through visual graphs and plots where various axis represent different data dimensions. However, this is far from the only way that we can interact with and explore data online. The term Sonification, or Auditory Displays, refers to the use of non-speech audio to convey information or perceptualize data (Hermann et.al, 2011). Using sonification methods can facilitate better accessibility practices and sometimes even be more efficient than data visualization techniques.

The research literature often differentiates among methods of sonification. For instance, the method of Audification refers to the direct playback of data samples or a direct translation of a data waveform into sound. So with audification, we can perhaps perceive patterns and irregularities in large datasets in record time.

However, most sonification applications augment existing visual-based data analysis tools, helping to extract valuable and coherent information in realtime. Model-based sonification is another and more advanced method where the goal is to investigate how acoustic responses from dynamic (sometimes visual) environments can provide valuable information in response to user interactions. As such, model-based sonification systems are great for explorative data analysis and augmenting existing visual analysis methods.

# Dataset

The dataset I am using for my examples is a simple, and quite generic, CSV spreadsheet detailing Arabic coffee quality from different regions and farms over several years. I sourced the dataset on Kaggle.com, where you can find lots of high-quality and free datasets for your projects, in a variety of different formats.

Here is a taste of what my dataset looks like:

<MyTable
  cols={[
    ["Country of Origin", "Brazil", "Brazil", "United States (Hawaii)"],
    ["Number of Bags", "300", "300", "25"],
    ["Year", "2010", "2010", "2010"],
    ["Owner.1", "JP Carneiro", "JP Carneiro", "Kona Pacific Farmers Co"],
  ]}
/>

Each of the 1300 rows in the spreadsheet represents a specific coffee shipment in a particular period between 2010 - 2017. On the other hand, the columns tell us information about the shipments, detailing everything from the quality of the coffee to the number of bags in the shipment.

It should be noted that I am only using a small fraction of this dataset in my examples in this post.

# Multi-dimensional Plot

Below is my first example of a model-based sonification system where I've augmented a traditional 2D plot with web audio. Each red dot reprents a row in the dataset. The height (vertical Y-axis) of the red dots equals coffee acidity levels over time (horizontal X-axis).

When you click the dots, the frequency of the played synth note represents the altitude of the farm. The higher the frequency of the note, the higher up the farm is.

<P5SonifyPlot />

One of the great things about p5 is that it includes a ton of Web Audio API utility functions for high-level declarative audio programming. This framework makes it easy to define and update audio parameters on the fly, making it ideal for designing interactive model-based sonification systems. Together with some basic knowledge of sound synthesis, these utilities make it possible to build complex synths, fast, that can react to user input or any other dynamic data.

The code block below demonstrates some key p5 audio utilities and how easy it is to update audio parameters.

```Javascript
// Example of a simple p5 sketch in instance mode.
// The frequency of a sinewave oscaillator is gradually
// incremented (from 0 to 10kHz), on every new window frame,
// resulting in a continuous sinesweep effect.

const sketch = (p) => {
  let sine;
  let myFreq;
  let threshold = 10000;

  p.setup = () => {
    p.createCanvas(200, 200);
    loadAudio();
  };

  p.draw = () => {
    updateAudio();
  };

  loadAudio() => {
    // load an oscillator with a sine-wave and set initial amplitude
    sine = new p5.Oscillator("sine");
    sine.amp(0.5);
    sine.freq(0);
    sine.start();
  }

  updateAudio() => {
    // update the frequency of the sine on every new window frame
    myFreq = myFreq > threshold ? 0 : myFreq += 1;
    sine.freq(myFreq);
  }
};

let myp5 = new p5(sketch);
```

But let's take it one step further. Below is an upgraded example of the red dot (now blue) coffee plot that features a second audio oscillator. The additional oscillator is used as an LFO to control the amplitude of the first oscillator, creating an amplitude modulation synthesizer in the process.

By listening to the frequency of the LFO and the varying characteristics of the synth notes, we can disseminate information about how many bags of coffee were produced by the given data point that year. The higher the frequency of the LFO, the more coffee bags were produced. See if you can hear the differences.

<P5SonifyPlotAmpMod />

But re-creating traditional plots and graph structures with p5 is tedious work. Unlike other libraries, such as Plotly, GraficaJs and Matplotlib, p5 is not optimized nor designed for data visualizations and statistical analysis. So instead of replicating traditional plotting techniques and functionalities, we should try to take advantage of the dynamic and strong visual capabilities of p5 to create more tailored platforms.

# The Sonic Orbs

Below is another example of a model-based sonification system with a more creative and object-oriented approach to visual and sonic design.

Here, users can navigate through time by interacting with a slider, moving colored orbs around in a 2D plot. Each orb represents a unique company in the dataset. Selected information about the company's coffee dispatches are mapped to various properties of the orb! In my example, the XY position of the orbs communicates the "balance" and "body" of coffee dispatches from 3 companies between 2010 - 2012. For simplicity's sake, I also made the sound of the orbs communicate the same thing, namely the "body" and "balance" qualities, using the same sound design as my multi-dimensional plot example, i.e. amplitude modulation.

<P5SonicOrbs />

But any data point can be mapped to any property of these orbs. In the code, I define an Orb class with some utility methods where each class instance represents a unique coffee owner/company from the dataset. This way, I know that every Orbs has the same basic features, such as shape, size, movement, sound, etc. Then, when I create a new orb, I simply pass whatever data I want to plot and sonify as arguments and ensure update that information in my p5 draw function based on the value of my slider.

```Javascript
// A simplified version of my Sonic Orb example as a p5 sketch in instance mode.
// ..

const sketch = (p) => {
  p.setup = () => {
    p.createCanvas(200, 200);
    loadOrbs();
  };

  p.draw = () => {
    orb_1.updatePos();
    orb_1.drawOrb();
    orb_1.makeSound();
    // etc..
  };

  loadOrbs() => {
    // these variables should come from a dataset. Hear instead, they are hard-coded.
    let owner_1 = "Kona Pacific Farmers Cooperative";
    let body = 20;
    let balance = 40;

    orb_1 = new Orb(owner_1, body, balance); // ++ other data points
    // etc..
  }
};

let myp5 = new p5(sketch);

class Orb {
  constructor(owner, body, balance) {
    this.owner = owner;
    this.position = [body, balance];
    this.oscillators = [];
    // etc..
  }
  updatePos(){}
  drawOrb() {}
  makeSound() {}
  // etc..
}
```

As you might have guessed, there are several advantages to using this object-oriented approach for data visualization and(!) sonification. First off, creating objects in code that can represent high-level data structures from your dataset is always a good programming strategy. A structure like this is highly scalable and maintainable for the future, making it more compatible when working with larger, varying, and more complex datasets.

But more importantly, using these objects to create complex sound textures and musical harmonies can really help generate meaningful content.

Together with looking at numbers or visually tracking objects on screen, this kind of combined visual and auditory sensing can benefit analysis work by giving the user a better impression of the data much quicker than usual. Personally speaking, it's also great for making comparisons and getting a sense of subtle differences between high-level data structures.
IKKE DEN OVER!

But more importantly, the Sonic Orb example leverages of our innate ability to listen, deconstruct, and differentiate amongst subtle complex sound textures and musical harmonies.

combining visual and auditory displays ...

Also in a browser setting? why is this cool?

The bad?

Why is this good? Why is it bad?

not as linear.

# Summary

# Source Code

# References

Hermann, T., Hunt, A., & Neuhoff, J. G. (Eds.). (2011). The Sonification Handbook (1st ed.). Logos Publishing House.
