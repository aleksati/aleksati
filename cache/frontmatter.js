export const frontMatterCache = [{"slug":"using-python-to-control-ableton-live","readingTime":{"text":"1 min read","minutes":0.81,"time":48600,"words":162},"title":"Using Python to Control Ableton Live","date":"2023-03-10","keywords":["audio","performance","ableton","python","midi"],"summary":"Inspired the musical and technical demands of my band's new liveset, I wrote some code to be able to navigate Ableton Live's session view like a 2D matrix using very few MIDI-controls. Here is how I did it."},{"slug":"fiveg-networked-music-performances","readingTime":{"text":"8 min read","minutes":7.155,"time":429300,"words":1431},"title":"5g Networked Music Performances","date":"2023-02-10","keywords":["audio","network","5g","nmp","research"],"summary":"Me and a colleague teamed up with Telenor Research to see what it takes to play music together over next generation 5g networks. Here are the preliminary results."},{"slug":"audiovideoanalysis","readingTime":{"text":"5 min read","minutes":4.505,"time":270300,"words":901},"title":"AudioVideoAnalysis","date":"2023-01-10","keywords":["video","audio","analysis","opengl","software-dev","max"],"summary":"A quick look into AudioVideoAnalysis v2.0; a standalone application for real-time OpenGL rendering of motiongrams and spectrograms, developed in collaboration with FourMS Lab at the RITMO research center, University of Oslo."},{"slug":"videoanalysis","readingTime":{"text":"7 min read","minutes":6.245,"time":374700,"words":1249},"title":"VideoAnalysis","date":"2022-12-10","keywords":["video","analysis","software-dev","max"],"summary":"A quick look into VideoAnalysis v2.1; a beginners tool for analyzing music-related body motion, developed in collaboration with FourMS Lab at the RITMO research centre, University of Oslo."}]