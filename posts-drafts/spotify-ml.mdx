---
title: "Musical Preference Recognition Using Spotify's Web API"
date: "2023-08-10"
keywords:
  [
    "spotify",
    "machine-learning",
    "audio",
    "data",
    "api",
    "computing",
    "analysis",
    "open-source",
  ]
type: "post"
summary: ""
---

PICTURE???

The music industry in the 21st century is all about data and streams. Companies like Spotify and Tidal use big data effectively to create personalized playlists and "smart" music recommendation systems to grab (and violently hold) our attention. Say what you will about Spotify, but I've been curious about their recommendation and preference recognition systems for some time due to their system performance and ability to reveal musical taste quantitatively like never before.

Recently, I learned that Spotify hosts a free and open Web API where you can access your streaming habits and explore complex audio analysis features from any track on their platform. This discovery led me to want to learn more about my musical preferences. I began to wonder whether I could build a custom preference recognition model using only data gathered from the Spotify API.

Althought many have attempted and documented similar projects before (links)

filtering approach (https://towardsdatascience.com/part-iii-building-a-song-recommendation-system-with-spotify-cf76b52705e7)

large dataset using spotipy (https://towardsdatascience.com/how-to-create-large-music-datasets-using-spotipy-40e7242cc6a6)

Spotify Recommendation System Using ML (https://thecleverprogrammer.com/2021/03/03/spotify-recommendation-system-with-machine-learning/)

collaborative filtering i in ML (https://thecleverprogrammer.com/2021/02/14/collaborative-filtering-in-machine-learning/)

few of them have used the their personal users listening data to kartlegge his/her own preference, and use that for learning.

This post details my attempt at building my own (tiny) music preference recognition model, a system that only uses the Spotify API as its data resource.

# Contents

1. <MyLink href="#dataset-acquisition">Dataset Acquisition</MyLink>
2. <MyLink href="#dataset-exploration">Dataset Exploration</MyLink>
3. <MyLink href="#the-models">The Models</MyLink>
4. <MyLink href="#results">Results</MyLink>
5. <MyLink href="#summary">Summing up</MyLink>

The project structure is divided into 3 main parts. To interface with the Spotify Web API, you need an account over at Spotify Dev for authentication. I also recommend using a Python library called Spotipy that effectively handles the API contact to Spotify endpoints. Using Spotipy frees up valuable time and code to navigate the intricate data structures returned by the API endpoints.

PICTURE OF DIAGRAM

---

# Dataset Acquisition

The most important part of my project was related to gathering and pre-processing data into a good dataset. To train a good recommendation model, simply gathering a bunch of tracks and features is not enough. You also need to provide unique scores (target values) associated with each item in the data that represents how much you enjoy each track. However, acquiring target values is not as straightforward as it sounds. Just think about it. How would you accurately and systematically rate a playlist of your favorite music? Is it even possible to precisely articulate our musical preferences?

## Target Distribution and Feature Selection

By leveraging a particular endpoint in the Spotify API, called _getTopTracks_, I was able to find an elegant solution to acquiring tracks and target values for my dataset. The _getTopTracks_ endpoint returns data about your short-, medium-, and long-term favorite tracks, which are essentially the ones you've played the most. Using this specific endpoint enabled me to do two things. First, gather a dataset of my favorite tracks, and second, leverage the fact that the tracks are delivered from the most-played to least-played and base my target values directly on the index position of each item in the data.

However, with data from different terms (short, medium, and long-term favorites), the question became whether all tracks should be rated equally or if some terms should carry higher ratings than others. To this end, I designed an algorithm that favored long-term over medium and short-term favorites. The algorithm gave all tracks a rating on a scale from 0 - 10, where 10 is the highest score, assigning long-term tracks scores between 10 - 4, medium-term tracks scores between 8-2, and short-term tracks between 6-0. Using a 0-10 scale helped me keep my options open for regression and classification-style learning later on. The algorithm created a fairly normally distributed curve of my target values, only with a slight preference for higher-rated tracks.

PICTURE OF DISTRIBUTION CURVE and DATASET FEATURES?

Typically, audio ML models are trained on fairly low-level audio analysis features, such as RMS values, MFCC, spectrogram, and spectral centroid. However, the most interesting aspect of the Spotify API is that it provides a series of high-level and unique audio descriptors that are directly related to musical activity, such as _danceability_, _valence_, and _instrumentalness_. These are all combinations of lower-level features and are most likely the product of complex modeling from Spotify devs. It's fair to say that to use these as features for machine learning is, by itself, an interesting venture.

Nevertheless, you'll also find more traditional and lower-level analysis features in the API, like _timbre_ and _pitches_, that are more similar to spectrogram and FFT audio representations. These are usually accessible as larger vectors and thus require some more pre-processing. To keep my options open, I included a variety of high vs. low, and scalar vs. vectorial Spotify features in my dataset. Amongst the collected were:

- Speechiness, Instrumentalness, Danceability, Valence, Loudness, Energy, Acousticness, Pitches, Timbre, Mode/key average, Duration, Tempo, Time signature

## Re-shaping

But my smugness was short-lived. I soon discovered that any API call to _getTopTracks_ has a strict 50-track limit, meaning I could only get a hold of 150 favorite tracks total (short, medium, and short-term tracks included). Here, the forums saved me. From reading online, I came across a small hack that enabled me to get twice as many tracks from each term, increasing my total track number to 300. Even better, I remembered a technique from uni for extending dataset sizes with noise to improve robustness and combat underfitting. Adding some noise to datasets can help ML models learn a better generalization of the data and improve the accuracy of the test set.

CODE EXAMPLE OF EXTENDING DATASET SIZES WITH NOISE

So, by supplying my initial 150-track dataset to an API hack, some noise extension, and duplicate removal, I ended up with about 4-500 tracks. This is still considered small for the recommandation tasks in question, but I figured I'd give it a shot anyway.

DATASET PICTURE

Scaling and stuff.

# Dataset Exploration

An interesting observation was that these high-level Spotify features did not immediately correlate with other suspected feature data. For instance, valence did not correlate particularly well with the mode of the tracks, as seen in the image below, something I anticipated.

Mode is the key. You would expect that the more minor songs are sader than major songs. Example that we can learn alot from just processing these kinds of data.

<figure style="float: none">
  <img
    src="/assets/image/2020_09_18_aleksati_ml_valence_mode.png"
    alt="Alternate Text"
    title="Image Title"
    width="620"
  />
  <figcaption>
    Valence (happiness) in relation to the average mode of all tracks in the
    dataset. The lower the mode value, the more minor keys occur in a specific
    track.
  </figcaption>
</figure>

# Das Model

Since the goal of my project was to train a system to recognize my taste in music, a personal rating from 1-10 was given to each track representing target values for the supervised MLP (neural network) regressor algorithm intend. Additionally, I used a repeated K-fold for training and validation because I feared that more strict dependencies on particular percentages of samples in each target class, like what stratified repeated K-fold ensures, could be problematic due to my shortage of training data.

Both principal component (PCA) and linear discriminate analysis (LDA) were considered as dimensionality reduction techniques for the dataset. By comparing the variance of different sets of features to the total feature variance, I could find an approximation of how many reduction components I would need:

```
#Here we find the minimal amount of DR-components needed
to keep 99% of the total feature variance.

feat_variance = np.var(features, axis=0).sum()
for i in range(features.shape[1]):
        temp = np.var(features[:,0:i+1], axis=0).sum()
        percentage = temp/feat_variance
        if percentage > 0.99:
            print("componenets needed: ", i+1)
            print("reached: ", percentage, "%")
            break
```

Even though the grid-search scores revealed a significant advantage of using LDA over PCA, I had to take into consideration that my system could be defined as a kind of hybrid between a classifier and a regressor. This is evident by examining the scalar nature and range of my target values. Therefore, the consequence of using LDA might be that it interprets my task as classification rather than regression, an interpretation that might compromise the validity of my algorithm. But even though PCA might have been more suited, its detrimental scores led me to use LDA instead and rather keep in mind the impact this might have on the model.

**LDA cross-validation grid-search results:**

```
best params: {'activation': 'logistic', 'hidden_layer_sizes': (2, 2), 'max_iter': 20000}
associated best score: 0.749
```

Lastly, the suggestion of using a logistic sigmoid activation function seemed logical considering that it's often used to predict normalized probabilities which is exactly what my model would do.

# Results

The general results were adequate and expected. However, with an R2 score of 0.75 on my limited dataset, I suspected signs of overfitting. In light of this, I conducted some tests by training the model on testing and training data before comparing the respective R2 and mean square errors. The larger the difference was between the comparisons, the more symptomatic my system would be of overfitting.

| Original Training Data |
| ---------------------- | -------- | ----------------------------------------------------- |
| Metric                 | Scores   | Difference between training and test data predictions |
| --------               | -------- | ---------------------------------------------------   |
| R2                     | 0.75     | 4-5%                                                  |
| MSE                    | -0.9     | 18-20%                                                |

As seen in the first table above, the first comparisons were not as bad as anticipated but still grounds for suspicion. To investigate further, I decided to artificially create more training data to test whether extending the size of the traing data could increase the performances and limit the metric differences.

| Original Training Data + 1/4 Artificially Created Training Data |
| --------------------------------------------------------------- | -------- | ----------------------------------------------------- |
| Metric                                                          | Scores   | Difference between training and test data predictions |
| --------                                                        | -------- | ---------------------------------------------------   |
| R2                                                              | 0.8      | 3%                                                    |
| MSE                                                             | -0.7     | 13-15%                                                |

At best, I only managed to decrease the differences by approximately 2-3\% when adding 1/4 of artificial training data. The lack of performance variation from extending the dataset in this way can indicate sub-optimal algorithm parameters, dataset quality and/or dataset size. Most likely due to the latter..

# Concluding Remarks

Evaluating performance metrics of a machine learning model depends on the system in question. I set out to prove that using Spotify data to train a machine learning model was feasible, and I believe this project has proved this concept.

The obvious dataset limitations were largely attributed to an inefficient process of gathering and labeling data. I believe that the system could have been better if the theme of the project was adjusted. For instance, comparisons of baroque composers or different musical genres could have facilitated a more effective data collection process. Alternatively, a more specialized focus, for instance on only piano sonatas or preludes, could also have yielded better results despite having a shortage of data.

For future work, it would be especially interesting to further explore and evaluate the high-level Spotify audio descriptors (danceability, valence etc..) through machine learning algorithms like this one.
