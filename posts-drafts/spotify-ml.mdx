---
title: "Musical Preference Recognition Using Spotify's Web API"
date: "2023-08-10"
keywords:
  [
    "spotify",
    "machine-learning",
    "audio",
    "data",
    "api",
    "computing",
    "analysis",
    "open-source",
  ]
type: "post"
summary: ""
---

PICTURE

Browsing the web in the 21st century is a bitter-sweet experience for those concerned about privacy. Our (browsing) time is money for companies mining our behavior and data for marketing revenue. But it's not all ads in the traditional sense. For instance, in the music industry, big data is used to create personalized playlists and "smart" music recommendation systems designed to give us the music we like on demand. Nobody is better at this than Spotify, currently the largest music streaming service in the known universe. For a long time, Spotify's recommendation systems have fascinated me due to their sheer performance and ability to reveal musical taste (quantitatively) like never before.

Recently, I learned that Spotify hosts a free Web API that lets any user access their streaming habits and complex high-level audio features from any track on their platform. Initially, this news got me excited about the possibility of using the API to learn more about my musical preferences. Later, I began to wonder whether I could build a custom preference recognition model using only data gathered from the API. This post details my attempt at building such a musical preferences recognition model, using only some Python code and the Spotify API as my resources.

# Contents

1. <MyLink href="#follow-the-code">Dataset Acquisition</MyLink>
2. <MyLink href="#follow-the-code">Dataset Exploration</MyLink>
3. <MyLink href="#why-sonify?">The Models</MyLink>
4. <MyLink href="#example-dataset">Results</MyLink>
5. <MyLink href="#summary">Summing up</MyLink>

---

# Dataset Acquisition

The project structure can be divided into .. parts, as demonstrated in the figure below. The first, and arguably most essential, part is concerned with gathering data and pre-processing it into a dataset for my models.

PICTURE OF DIAGRAMS

To interface with the Spotify Web API, you need an account over at Spotify Dev for authentication. To start building your dataset, I recommend using a Python library called Spotipy that effectively handles the API contact to Spotify endpoints. Using this library will free up time to focus on navigating the sometimes intricate data structures the servers return.

But simply gathering a bunch of track data is not enough. Predicting musical preferences requires datasets where each item is associated with a rating or target value, basically how much you enjoy each track. However, this rating process is not as straightforward as it sounds. Just think about it. How would you accurately and systematically rate a playlist of your favorite music? Is it even possible to precisely articulate our musical preferences? And if we want our models to be good, we're talking about rating hundreds, maybe thousands, of tracks. With these facts in mind, I knew that manually rating each track in a playlist was out of the question. I had to find a more clever way.

Luckily, after some tinkering, I stumbled over a possible solution to my target issue. By using an endpoint called "getTopTracks", Spotify will give you a user's favorite tracks (most played) from three different time ranges, namely "short term", "medium term", and "long term". Even better, the favorite tracks are delivered in chronological order, meaning the most-played favorite track is always the first item, and the least-played favorite track is always the last. However, my initial rejoicing was short-lived. After a short while, I discovered a track limit of 50 tracks on any "getTopTracks" API calls. This means that if you retrieve every track you can get, from all time ranges (long, medium, and short-term favorites), it will only amount to 150 tracks in total. And even worse, there's no anti-duplicate guarantee, meaning tracks in the short-term favorites can also be part of your long-term favorites. For a decent model, I would need many times more tracks in my dataset.

PICTURE HERE

Hack to make it 300, would try to add noise to extend the dataset into 5-600 items.

## Rating Distribution and Feature Selection

how did I rate them.
Also turned into a classification model

the features I collected from each track. Name everything. In table.

removed duplicates

Pre-processing

# Dataset Exploration

My dataset conisted of 100-200++ songs of Bach on Spotify. I went through all tracks into a playlist
and gave each individual track a rating from 0 - 10, where 0 is the lowest rating.
Together with the audio features, these target values...
(into a seperate csv. file)

extract features.
to able to easily speak with the Spotify Web API
use spotipy, the pip package. ! in connection with the web api. this is all possible becaue of those.

describe the difference between high and low level features.

You need to .. aut. then .. Lots of great tutorials out there for you to learn how to do this.

BUT, you can check out my full feature extraction python notebook from my github.

SHOW SIMPLE CODE EXAMPLE.

Since I was new to this and wanted to explore
Some scalar features, and some vectorial (many values per item)... usually a values per segment of the track. so you would have one value for verse 1, another for chrous etc.
Also many normal features combined with high-level Spotify features.

I decided to collect a variety of features for experimentation, including a few high-level features native to Spotify's ecosystem; duration, energy, tempo, time signature, loudness, **_pitches_**, **_valence_**, **_danceability_**, **_timbres_**, key and mode.

These are the high-level ones. More interesting ones!

- **Pitches** - The pitch feature returns a normalized vector per segment (a consistent subdivision of a track based on a consistent amount of sound) with 12 values representing the degree of occurrence for each note. I collected every pitch vector of a given track and averaged corresponding indexes. What remained was one list per track with the average occurrence of each note in the track.
- **Timbre** - Similar to pitches, the timbre feature returns a vector per segment with 12 elements. The specifics on what this timbre feature is based remains ambiguous, but it refers to the quality of the notes in the segment. The API also presses that these timbre values are best compared to each other. The gathering process here was the as with the pitch features.
- **Danceability** - This measure refers to how suitable a track is for dancing based on a combination of tempo, rhythm, stability, beat strength etc. Returns a scalar value per track.
- **Valence** - A normalized scalar of the "positiveness" conveyed by a track. The higher this value, the more happy, cheerful and euphoric the track is. The lower the value, the more sad, depressing and angry the track is.

Show how the dataset looked in the end with a table!

An interesting observation was that these high-level Spotify features did not immediately correlate with other suspected feature data. For instance, valence did not correlate particularly well with the mode of the tracks, as seen in the image below, something I anticipated.

Mode is the key. You would expect that the more minor songs are sader than major songs. Example that we can learn alot from just processing these kinds of data.

<figure style="float: none">
  <img
    src="/assets/image/2020_09_18_aleksati_ml_valence_mode.png"
    alt="Alternate Text"
    title="Image Title"
    width="620"
  />
  <figcaption>
    Valence (happiness) in relation to the average mode of all tracks in the
    dataset. The lower the mode value, the more minor keys occur in a specific
    track.
  </figcaption>
</figure>

# Das Model

Since the goal of my project was to train a system to recognize my taste in music, a personal rating from 1-10 was given to each track representing target values for the supervised MLP (neural network) regressor algorithm intend. Additionally, I used a repeated K-fold for training and validation because I feared that more strict dependencies on particular percentages of samples in each target class, like what stratified repeated K-fold ensures, could be problematic due to my shortage of training data.

Both principal component (PCA) and linear discriminate analysis (LDA) were considered as dimensionality reduction techniques for the dataset. By comparing the variance of different sets of features to the total feature variance, I could find an approximation of how many reduction components I would need:

```
#Here we find the minimal amount of DR-components needed
to keep 99% of the total feature variance.

feat_variance = np.var(features, axis=0).sum()
for i in range(features.shape[1]):
        temp = np.var(features[:,0:i+1], axis=0).sum()
        percentage = temp/feat_variance
        if percentage > 0.99:
            print("componenets needed: ", i+1)
            print("reached: ", percentage, "%")
            break
```

Even though the grid-search scores revealed a significant advantage of using LDA over PCA, I had to take into consideration that my system could be defined as a kind of hybrid between a classifier and a regressor. This is evident by examining the scalar nature and range of my target values. Therefore, the consequence of using LDA might be that it interprets my task as classification rather than regression, an interpretation that might compromise the validity of my algorithm. But even though PCA might have been more suited, its detrimental scores led me to use LDA instead and rather keep in mind the impact this might have on the model.

**LDA cross-validation grid-search results:**

```
best params: {'activation': 'logistic', 'hidden_layer_sizes': (2, 2), 'max_iter': 20000}
associated best score: 0.749
```

Lastly, the suggestion of using a logistic sigmoid activation function seemed logical considering that it's often used to predict normalized probabilities which is exactly what my model would do.

# Results

The general results were adequate and expected. However, with an R2 score of 0.75 on my limited dataset, I suspected signs of overfitting. In light of this, I conducted some tests by training the model on testing and training data before comparing the respective R2 and mean square errors. The larger the difference was between the comparisons, the more symptomatic my system would be of overfitting.

| Original Training Data |
| ---------------------- | -------- | ----------------------------------------------------- |
| Metric                 | Scores   | Difference between training and test data predictions |
| --------               | -------- | ---------------------------------------------------   |
| R2                     | 0.75     | 4-5%                                                  |
| MSE                    | -0.9     | 18-20%                                                |

As seen in the first table above, the first comparisons were not as bad as anticipated but still grounds for suspicion. To investigate further, I decided to artificially create more training data to test whether extending the size of the traing data could increase the performances and limit the metric differences.

| Original Training Data + 1/4 Artificially Created Training Data |
| --------------------------------------------------------------- | -------- | ----------------------------------------------------- |
| Metric                                                          | Scores   | Difference between training and test data predictions |
| --------                                                        | -------- | ---------------------------------------------------   |
| R2                                                              | 0.8      | 3%                                                    |
| MSE                                                             | -0.7     | 13-15%                                                |

At best, I only managed to decrease the differences by approximately 2-3\% when adding 1/4 of artificial training data. The lack of performance variation from extending the dataset in this way can indicate sub-optimal algorithm parameters, dataset quality and/or dataset size. Most likely due to the latter..

# Concluding Remarks

Evaluating performance metrics of a machine learning model depends on the system in question. I set out to prove that using Spotify data to train a machine learning model was feasible, and I believe this project has proved this concept.

The obvious dataset limitations were largely attributed to an inefficient process of gathering and labeling data. I believe that the system could have been better if the theme of the project was adjusted. For instance, comparisons of baroque composers or different musical genres could have facilitated a more effective data collection process. Alternatively, a more specialized focus, for instance on only piano sonatas or preludes, could also have yielded better results despite having a shortage of data.

For future work, it would be especially interesting to further explore and evaluate the high-level Spotify audio descriptors (danceability, valence etc..) through machine learning algorithms like this one.
