---
title: "Musical Preference Recognition Using Spotify's Web API"
date: "2023-08-10"
keywords:
  [
    "spotify",
    "machine-learning",
    "audio",
    "data",
    "api",
    "computing",
    "analysis",
    "open-source",
  ]
type: "post"
summary: ""
---

PICTURE???

The music industry in the 21st century is all about data and streams. Companies like Spotify and Tidal use big data effectively to create personalized playlists and "smart" music recommendation systems to grab (and violently hold) our attention. Say what you will about Spotify, but I've been curious about their recommendation and preference recognition systems for some time due to their system performance and ability to reveal musical taste quantitatively like never before.

Recently, I learned that Spotify hosts a free and open Web API where you can access your streaming habits and explore complex audio analysis features from any track on their platform. This discovery led me to want to learn more about my musical preferences. I began to wonder whether I could build a custom preference recognition model using only data gathered from the Spotify API.

This post details my attempt at building my own (tiny) music preference recognition model, a system that only uses the Spotify API as its data resource.

# Contents

1. <MyLink href="#dataset-acquisition">Dataset Acquisition</MyLink>
2. <MyLink href="#dataset-exploration">Dataset Exploration</MyLink>
3. <MyLink href="#the-models">The Models</MyLink>
4. <MyLink href="#results">Results</MyLink>
5. <MyLink href="#summary">Summing up</MyLink>

The project structure is divided into 3 main parts. To interface with the Spotify Web API, you need an account over at Spotify Dev for authentication. I also recommend using a Python library called Spotipy that effectively handles the API contact to Spotify endpoints. Using Spotipy frees up valuable time and code to navigate the intricate data structures returned by the API endpoints.

PICTURE OF DIAGRAM

---

# Dataset Acquisition

The most important part of my project was related to gathering and pre-processing data into a good dataset. To train a good recommendation model, simply gathering a bunch of tracks and features is not enough. You also need to provide unique scores (target values) associated with each item in the data that represents how much you enjoy each track. However, acquiring target values is not as straightforward as it sounds. Just think about it. How would you accurately and systematically rate a playlist of your favorite music? Is it even possible to precisely articulate our musical preferences?

## Target Distribution and Feature Selection

By leveraging a particular endpoint in the Spotify API, called _getTopTracks_, I was able to find an elegant solution to acquiring tracks and target values for my dataset. The _getTopTracks_ endpoint returns data about your short-, medium-, and long-term favorite tracks, which are essentially the ones you've played the most. Using this specific endpoint enabled me to do two things. First, gather a dataset of my favorite tracks, and second, leverage the fact that the tracks are delivered from the most-played to least-played and base my target values directly on the index position of each item in the data.

However, with data from different terms (short, medium, and long-term favorites), the question became whether tracks in each terms should be rated equally or if some terms should carry higher ratings than others. To this end, I designed an algorithm that favored long-term over medium and short-term favorites. The algorithm gave all tracks a rating on a scale from 0 - 10, where 10 is the highest score, assigning long-term tracks scores between 10 - 4, medium-term tracks scores between 8-2, and short-term tracks between 6-0. Using a 0-10 scale helped me keep my options open for regression and classification-style learning later on. The result of this algorithm was a fairly normally distributed curve of ratings with a slight preference for higher-rated tracks.

PICTURE OF DISTRIBUTION CURVE

features ....

## Re-shaping

But my smugness was short-lived. I soon discovered that any API call to _getTopTracks_ has a strict 50-track limit, meaning I could only get a hold of 150 favorite tracks total (short, medium, and short-term tracks included). Here, the forums saved me. From reading online, I came across a small hack that enabled me to get twice as many tracks from each term, increasing my total track number to 300. Even better, I remembered a technique from uni for extending dataset sizes with noise to improve robustness and combat underfitting. Adding some noise to datasets can help ML models learn a better generalization of the data and improve the accuracy of the test set.

CODE EXAMPLE OF EXTENDING DATASET SIZES WITH NOISE

So, by supplying my initial 150-track dataset to an API hack, some noise extension, and duplicate removal, I ended up with about 4-500 tracks. This is still considered small for the recommandation tasks in question, but I figured I'd give it a shot anyway.

DATASET PICTURE

Scaling and stuff.

# Dataset Exploration

My dataset conisted of 100-200++ songs of Bach on Spotify. I went through all tracks into a playlist
and gave each individual track a rating from 0 - 10, where 0 is the lowest rating.
Together with the audio features, these target values...
(into a seperate csv. file)

extract features.
to able to easily speak with the Spotify Web API
use spotipy, the pip package. ! in connection with the web api. this is all possible becaue of those.

describe the difference between high and low level features.

You need to .. aut. then .. Lots of great tutorials out there for you to learn how to do this.

BUT, you can check out my full feature extraction python notebook from my github.

SHOW SIMPLE CODE EXAMPLE.

Since I was new to this and wanted to explore
Some scalar features, and some vectorial (many values per item)... usually a values per segment of the track. so you would have one value for verse 1, another for chrous etc.
Also many normal features combined with high-level Spotify features.

I decided to collect a variety of features for experimentation, including a few high-level features native to Spotify's ecosystem; duration, energy, tempo, time signature, loudness, **_pitches_**, **_valence_**, **_danceability_**, **_timbres_**, key and mode.

These are the high-level ones. More interesting ones!

- **Pitches** - The pitch feature returns a normalized vector per segment (a consistent subdivision of a track based on a consistent amount of sound) with 12 values representing the degree of occurrence for each note. I collected every pitch vector of a given track and averaged corresponding indexes. What remained was one list per track with the average occurrence of each note in the track.
- **Timbre** - Similar to pitches, the timbre feature returns a vector per segment with 12 elements. The specifics on what this timbre feature is based remains ambiguous, but it refers to the quality of the notes in the segment. The API also presses that these timbre values are best compared to each other. The gathering process here was the as with the pitch features.
- **Danceability** - This measure refers to how suitable a track is for dancing based on a combination of tempo, rhythm, stability, beat strength etc. Returns a scalar value per track.
- **Valence** - A normalized scalar of the "positiveness" conveyed by a track. The higher this value, the more happy, cheerful and euphoric the track is. The lower the value, the more sad, depressing and angry the track is.

Show how the dataset looked in the end with a table!

An interesting observation was that these high-level Spotify features did not immediately correlate with other suspected feature data. For instance, valence did not correlate particularly well with the mode of the tracks, as seen in the image below, something I anticipated.

Mode is the key. You would expect that the more minor songs are sader than major songs. Example that we can learn alot from just processing these kinds of data.

<figure style="float: none">
  <img
    src="/assets/image/2020_09_18_aleksati_ml_valence_mode.png"
    alt="Alternate Text"
    title="Image Title"
    width="620"
  />
  <figcaption>
    Valence (happiness) in relation to the average mode of all tracks in the
    dataset. The lower the mode value, the more minor keys occur in a specific
    track.
  </figcaption>
</figure>

# Das Model

Since the goal of my project was to train a system to recognize my taste in music, a personal rating from 1-10 was given to each track representing target values for the supervised MLP (neural network) regressor algorithm intend. Additionally, I used a repeated K-fold for training and validation because I feared that more strict dependencies on particular percentages of samples in each target class, like what stratified repeated K-fold ensures, could be problematic due to my shortage of training data.

Both principal component (PCA) and linear discriminate analysis (LDA) were considered as dimensionality reduction techniques for the dataset. By comparing the variance of different sets of features to the total feature variance, I could find an approximation of how many reduction components I would need:

```
#Here we find the minimal amount of DR-components needed
to keep 99% of the total feature variance.

feat_variance = np.var(features, axis=0).sum()
for i in range(features.shape[1]):
        temp = np.var(features[:,0:i+1], axis=0).sum()
        percentage = temp/feat_variance
        if percentage > 0.99:
            print("componenets needed: ", i+1)
            print("reached: ", percentage, "%")
            break
```

Even though the grid-search scores revealed a significant advantage of using LDA over PCA, I had to take into consideration that my system could be defined as a kind of hybrid between a classifier and a regressor. This is evident by examining the scalar nature and range of my target values. Therefore, the consequence of using LDA might be that it interprets my task as classification rather than regression, an interpretation that might compromise the validity of my algorithm. But even though PCA might have been more suited, its detrimental scores led me to use LDA instead and rather keep in mind the impact this might have on the model.

**LDA cross-validation grid-search results:**

```
best params: {'activation': 'logistic', 'hidden_layer_sizes': (2, 2), 'max_iter': 20000}
associated best score: 0.749
```

Lastly, the suggestion of using a logistic sigmoid activation function seemed logical considering that it's often used to predict normalized probabilities which is exactly what my model would do.

# Results

The general results were adequate and expected. However, with an R2 score of 0.75 on my limited dataset, I suspected signs of overfitting. In light of this, I conducted some tests by training the model on testing and training data before comparing the respective R2 and mean square errors. The larger the difference was between the comparisons, the more symptomatic my system would be of overfitting.

| Original Training Data |
| ---------------------- | -------- | ----------------------------------------------------- |
| Metric                 | Scores   | Difference between training and test data predictions |
| --------               | -------- | ---------------------------------------------------   |
| R2                     | 0.75     | 4-5%                                                  |
| MSE                    | -0.9     | 18-20%                                                |

As seen in the first table above, the first comparisons were not as bad as anticipated but still grounds for suspicion. To investigate further, I decided to artificially create more training data to test whether extending the size of the traing data could increase the performances and limit the metric differences.

| Original Training Data + 1/4 Artificially Created Training Data |
| --------------------------------------------------------------- | -------- | ----------------------------------------------------- |
| Metric                                                          | Scores   | Difference between training and test data predictions |
| --------                                                        | -------- | ---------------------------------------------------   |
| R2                                                              | 0.8      | 3%                                                    |
| MSE                                                             | -0.7     | 13-15%                                                |

At best, I only managed to decrease the differences by approximately 2-3\% when adding 1/4 of artificial training data. The lack of performance variation from extending the dataset in this way can indicate sub-optimal algorithm parameters, dataset quality and/or dataset size. Most likely due to the latter..

# Concluding Remarks

Evaluating performance metrics of a machine learning model depends on the system in question. I set out to prove that using Spotify data to train a machine learning model was feasible, and I believe this project has proved this concept.

The obvious dataset limitations were largely attributed to an inefficient process of gathering and labeling data. I believe that the system could have been better if the theme of the project was adjusted. For instance, comparisons of baroque composers or different musical genres could have facilitated a more effective data collection process. Alternatively, a more specialized focus, for instance on only piano sonatas or preludes, could also have yielded better results despite having a shortage of data.

For future work, it would be especially interesting to further explore and evaluate the high-level Spotify audio descriptors (danceability, valence etc..) through machine learning algorithms like this one.
